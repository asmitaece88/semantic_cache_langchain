{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d3ba2d",
   "metadata": {},
   "source": [
    "# Working with Redis for Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfb33b",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use the RedisCache and RedisSemanticCache classes from the langchain-redis package to implement caching for LLM responses using Redis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7380cf",
   "metadata": {},
   "source": [
    "# installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdd3e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ipywidgets\n",
    "# %pip install langchain-core\n",
    "# %pip install langchain-redis\n",
    "# %pip install langchain-openai\n",
    "# %pip install redis\n",
    "#%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fde63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f32964",
   "metadata": {},
   "source": [
    "# importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c948729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import redis\n",
    "\n",
    "#from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_redis import RedisCache, RedisSemanticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1be29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.cache import InMemoryCache\n",
    "import langchain\n",
    "\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b0b4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a040f",
   "metadata": {},
   "source": [
    "# setting up  redis connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74a28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIS_URL = \"redis://default:lo1hLY1c0erZN4pLWIFmJeB2UGaVTY7e@redis-16842.c80.us-east-1-2.ec2.cloud.redislabs.com:16842\"\n",
    "# redis_client = redis.from_url(REDIS_URL)\n",
    "# redis_client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1859dfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redis_client = redis.from_url(REDIS_URL)\n",
    "redis_client.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf19ed35",
   "metadata": {},
   "source": [
    "# this means redis db is  working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c2c4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4790d",
   "metadata": {},
   "source": [
    "# Using Redis as a Standard Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b26dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import langchain\n",
    "\n",
    "from langchain_community.cache import RedisCache, RedisSemanticCache\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "#REDIS_URL = \"redis://localhost:6379\"  # change if needed\n",
    "\n",
    "# Use a chat model (recommended in new LangChain)\n",
    "llm = ChatOpenAI(temperature=0)  # uses OPENAI_API_KEY from env\n",
    "\n",
    "def execute_with_timing(prompt: str):\n",
    "    start_time = time.time()\n",
    "    response = llm.invoke(prompt)  # returns an AIMessage\n",
    "    end_time = time.time()\n",
    "    return response.content, end_time - start_time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfcd172e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'redis://default:lo1hLY1c0erZN4pLWIFmJeB2UGaVTY7e@redis-16842.c80.us-east-1-2.ec2.cloud.redislabs.com:16842'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REDIS_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d88c5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIS_URL = \"redis://default:lo1hLY1c0erZN4pLWIFmJeB2UGaVTY7e@redis-16842.c80.us-east-1-2.ec2.cloud.redislabs.com:16842\"\n",
    "\n",
    "redis_client = redis.Redis.from_url(\n",
    "    REDIS_URL,\n",
    "    decode_responses=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdc276de",
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_cache = RedisCache(redis_=redis_client)\n",
    "\n",
    "langchain.llm_cache = redis_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c8d8e",
   "metadata": {},
   "source": [
    "# traditional cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cedf856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (not cached):\n",
      "Caching is the process of storing frequently accessed data in a temporary storage area to improve performance. When a user requests data that has been previously cached, it can be retrieved quickly without having to access the original source. Caching helps reduce load times and improve overall system efficiency by reducing the need to repeatedly fetch data from its original location.\n",
      "Time: 1.05 seconds\n",
      "\n",
      "Second call (cached):\n",
      "Caching is the process of storing frequently accessed data in a temporary storage area to improve performance. When a user requests data that is already cached, it can be retrieved quickly without having to access the original source. Caching helps reduce load times and improve overall system efficiency by reducing the need to repeatedly fetch data from its original location.\n",
      "Time: 1.13 seconds\n",
      "\n",
      "Speed improvement: 0.92x faster\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "prompt = \"Explain the concept of caching in three sentences.\"\n",
    "\n",
    "# First call (not cached)\n",
    "result1, time1 = execute_with_timing(prompt)\n",
    "print(\"First call (not cached):\")\n",
    "print(f\"{result1}\\nTime: {time1:.2f} seconds\\n\")\n",
    "\n",
    "# Second call (should be cached)\n",
    "result2, time2 = execute_with_timing(prompt)\n",
    "print(\"Second call (cached):\")\n",
    "print(f\"{result2}\\nTime: {time2:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time2:.2f}x faster\\n\")\n",
    "\n",
    "# # Clear the cache\n",
    "# redis_cache.clear()\n",
    "# print(\"Cache cleared (standard Redis cache)\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f038373",
   "metadata": {},
   "source": [
    "# using rediscache as a semantic cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edaa1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "semantic_cache = RedisSemanticCache(\n",
    "    redis_url=REDIS_URL, embedding=embeddings, score_threshold=0.2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7405ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.llm_cache = semantic_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42802afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.cache.RedisSemanticCache at 0x1f522afd160>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain.llm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c72cdb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query:\n",
      "Prompt: What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "Time: 0.73 seconds\n",
      "\n",
      "Similar query:\n",
      "Prompt: Can you tell me the capital city of France?\n",
      "\n",
      "The capital city of France is Paris.\n",
      "Time: 1.64 seconds\n",
      "\n",
      "Speed improvement: 0.45x faster\n"
     ]
    }
   ],
   "source": [
    "# Original prompt\n",
    "original_prompt = \"What is the capital of France?\"\n",
    "result1, time1 = execute_with_timing(original_prompt)\n",
    "print(f\"Original query:\\nPrompt: {original_prompt}\\n\")\n",
    "print(f\"{result1}\\nTime: {time1:.2f} seconds\\n\")\n",
    "\n",
    "# Semantically similar prompt\n",
    "similar_prompt = \"Can you tell me the capital city of France?\"\n",
    "result2, time2 = execute_with_timing(similar_prompt)\n",
    "print(f\"Similar query:\\nPrompt: {similar_prompt}\\n\")\n",
    "print(f\"{result2}\\nTime: {time2:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time2:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14961303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar query:\n",
      "Prompt: Hey , quickly tell e capital cty of france?\n",
      "\n",
      "The capital city of France is Paris.\n",
      "Time: 0.65 seconds\n",
      "\n",
      "Speed improvement: 1.13x faster\n"
     ]
    }
   ],
   "source": [
    "# Semantically similar prompt\n",
    "similar_prompt_2 = \"Hey , quickly tell e capital cty of france?\"\n",
    "result3, time3 = execute_with_timing(similar_prompt_2)\n",
    "print(f\"Similar query:\\nPrompt: {similar_prompt_2}\\n\")\n",
    "print(f\"{result3}\\nTime: {time3:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time3:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b97b153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar query:\n",
      "Prompt: Hey , quickly tell e capital cty of france?\n",
      "\n",
      "The capital city of France is Paris.\n",
      "Time: 0.69 seconds\n",
      "\n",
      "Speed improvement: 1.06x faster\n"
     ]
    }
   ],
   "source": [
    "similar_prompt_3 = \"Hey , quickly tell e capital cty of france?\"\n",
    "result4, time4 = execute_with_timing(similar_prompt_3)\n",
    "print(f\"Similar query:\\nPrompt: {similar_prompt_3}\\n\")\n",
    "print(f\"{result4}\\nTime: {time4:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time4:.2f}x faster\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
